{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d66530-b4e2-4b47-9413-92240069c5e1",
   "metadata": {},
   "source": [
    "# Chain of Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652e68d-4278-4e8f-9132-6ba90905d073",
   "metadata": {},
   "source": [
    "Chain of thought is a prompting technique introduced in the paper [\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"](https://arxiv.org/abs/2201.11903) where throught prompting the authors generate a series of intermediate reasoning steps which improves the ability of LLMs to perform complex reasoning.\n",
    "\n",
    "In this guide, we use [outlines](https://outlines-dev.github.io/outlines/) to apply chain of thought through structured output with the quantized `Hermes-2-Pro-Llama-3-8B`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f30dda-18f6-415e-ac23-e87aeb636f83",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Install llama-cpp-python and outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467c6655-4dd6-4f4e-ad9b-42fc1de0f52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:05.655426Z",
     "iopub.status.busy": "2024-08-08T16:26:05.654799Z",
     "iopub.status.idle": "2024-08-08T16:26:05.661046Z",
     "shell.execute_reply": "2024-08-08T16:26:05.659644Z",
     "shell.execute_reply.started": "2024-08-08T16:26:05.655375Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN IT ONLY ONCE TO INSTALL THE REQUIREMENTS\n",
    "# %pip install llama-cpp-python outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da62069-96d7-43e5-8968-64b48bc1384b",
   "metadata": {},
   "source": [
    "For detailed installation instructions, see [llama-cpp-python installation](https://llama-cpp-python.readthedocs.io/en/stable/) and [outlines installation](https://outlines-dev.github.io/outlines/installation/)\n",
    "\n",
    "### Pull the model from HuggingFace\n",
    "\n",
    "Download a GGUF model from HuggingFace [here](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/tree/main), for example, the Q4_K_M one (it requires 4.92 GB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6e9b01-0ad9-4f40-ac99-2e9340c1d3b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:05.662881Z",
     "iopub.status.busy": "2024-08-08T16:26:05.662450Z",
     "iopub.status.idle": "2024-08-08T16:26:05.686433Z",
     "shell.execute_reply": "2024-08-08T16:26:05.685275Z",
     "shell.execute_reply.started": "2024-08-08T16:26:05.662839Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN IT ONLY ONCE TO DOWNLOAD THE GGUF MODEL, IN THIS CASE THE Q4_K_M\n",
    "# !wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fa3ae-28bf-4636-9f23-92b3204df17d",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Chain of Thought\n",
    "\n",
    "### Define Pydantic class\n",
    "\n",
    "We first define our Pydantic class for a reasoning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96cdbccc-c584-4966-a442-02741a171ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:05.688514Z",
     "iopub.status.busy": "2024-08-08T16:26:05.688043Z",
     "iopub.status.idle": "2024-08-08T16:26:05.813020Z",
     "shell.execute_reply": "2024-08-08T16:26:05.811943Z",
     "shell.execute_reply.started": "2024-08-08T16:26:05.688469Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Reasoning_Step(BaseModel):\n",
    "    reasoning_step: str = Field(..., description=\"Reasoning step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691514f-094c-4b5f-8065-71caddd23ca7",
   "metadata": {},
   "source": [
    "We then define the Pydantic class for reasoning which will consist of a list of reasoning steps and a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532995c5-f076-4bf7-b294-ed70332c1c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:05.814802Z",
     "iopub.status.busy": "2024-08-08T16:26:05.814393Z",
     "iopub.status.idle": "2024-08-08T16:26:05.823195Z",
     "shell.execute_reply": "2024-08-08T16:26:05.822209Z",
     "shell.execute_reply.started": "2024-08-08T16:26:05.814762Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Reasoning(BaseModel):\n",
    "    reasoning: List[Reasoning_Step] = Field(..., description=\"List of reasoning steps\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83358efd-2411-4383-962e-109b9d8afcc8",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f2fe73-56f7-4533-9357-394d5c6555dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:05.825224Z",
     "iopub.status.busy": "2024-08-08T16:26:05.824745Z",
     "iopub.status.idle": "2024-08-08T16:26:14.517480Z",
     "shell.execute_reply": "2024-08-08T16:26:14.516116Z",
     "shell.execute_reply.started": "2024-08-08T16:26:05.825179Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "from llama_cpp import Llama\n",
    "from outlines import generate, models\n",
    "\n",
    "llm = Llama(\n",
    "    \"/big_storage/llms/models/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n",
    "    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n",
    "        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
    "    ),\n",
    "    n_gpu_layers=-1,\n",
    "    flash_attn=True,\n",
    "    n_ctx=8192,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model = models.LlamaCpp(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33f0a08-a699-4682-a50a-e5b21acb7645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:14.519387Z",
     "iopub.status.busy": "2024-08-08T16:26:14.519194Z",
     "iopub.status.idle": "2024-08-08T16:26:14.522935Z",
     "shell.execute_reply": "2024-08-08T16:26:14.522297Z",
     "shell.execute_reply.started": "2024-08-08T16:26:14.519372Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # ignore runtime warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91149a2f-0d15-4d8f-8827-73a15a38464f",
   "metadata": {},
   "source": [
    "We build a regex from the `Reasoning` Pydantic class which the model will be forced to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a125eb20-efb2-4274-a42d-a35f58d9db54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:14.523680Z",
     "iopub.status.busy": "2024-08-08T16:26:14.523509Z",
     "iopub.status.idle": "2024-08-08T16:26:14.563053Z",
     "shell.execute_reply": "2024-08-08T16:26:14.562077Z",
     "shell.execute_reply.started": "2024-08-08T16:26:14.523666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\{ \"reasoning\" : \\\\[ ((\\\\{ \"reasoning_step\" : \"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\" \\\\})(, (\\\\{ \"reasoning_step\" : \"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\" \\\\})){0,})? \\\\] , \"conclusion\" : \"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\" \\\\}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from outlines.integrations.utils import convert_json_schema_to_str\n",
    "from outlines.fsm.json_schema import build_regex_from_schema\n",
    "\n",
    "json_schema = Reasoning.model_json_schema()\n",
    "schema_str = convert_json_schema_to_str(json_schema=json_schema)\n",
    "regex_str = build_regex_from_schema(schema_str, whitespace_pattern=r\" \")\n",
    "regex_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853e5b6-1a40-43aa-a03d-4d5ece02b9c7",
   "metadata": {},
   "source": [
    "We then need to adapt our prompt to the [Hermes prompt format for JSON schema](https://github.com/NousResearch/Hermes-Function-Calling?tab=readme-ov-file#prompt-format-for-json-mode--structured-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fddb1a4-fc3d-4a81-bfb6-ab07c94c16e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:14.564898Z",
     "iopub.status.busy": "2024-08-08T16:26:14.564446Z",
     "iopub.status.idle": "2024-08-08T16:26:14.570846Z",
     "shell.execute_reply": "2024-08-08T16:26:14.569634Z",
     "shell.execute_reply.started": "2024-08-08T16:26:14.564854Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_hermes_prompt(question):\n",
    "    return (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a world class AI model who answers questions in JSON with correct Pydantic schema. \"\n",
    "        \"Here's the json schema you must adhere to:\\n<schema>\\n\" + str(json_schema) + \"\\n</schema>\"\n",
    "        \"\\n<|im_start|>user\\n\" + question + \"<|im_end|>\"\n",
    "        \"\\n<|im_start|>assistant\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133966dc-642a-4c64-983a-66f0ece78b2b",
   "metadata": {},
   "source": [
    "For a given `user_prompt` we obtain the hermes prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939e5c40-81ce-4a5a-b253-480e1a0bb1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:14.572631Z",
     "iopub.status.busy": "2024-08-08T16:26:14.572211Z",
     "iopub.status.idle": "2024-08-08T16:26:14.588481Z",
     "shell.execute_reply": "2024-08-08T16:26:14.587336Z",
     "shell.execute_reply.started": "2024-08-08T16:26:14.572591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a world class AI model who answers questions in JSON with correct Pydantic schema. Here's the json schema you must adhere to:\n",
      "<schema>\n",
      "{'$defs': {'Reasoning_Step': {'properties': {'reasoning_step': {'description': 'Reasoning step', 'title': 'Reasoning Step', 'type': 'string'}}, 'required': ['reasoning_step'], 'title': 'Reasoning_Step', 'type': 'object'}}, 'properties': {'reasoning': {'description': 'List of reasoning steps', 'items': {'$ref': '#/$defs/Reasoning_Step'}, 'title': 'Reasoning', 'type': 'array'}, 'conclusion': {'description': 'Conclusion', 'title': 'Conclusion', 'type': 'string'}}, 'required': ['reasoning', 'conclusion'], 'title': 'Reasoning', 'type': 'object'}\n",
      "</schema>\n",
      "<|im_start|>user\n",
      "9.11 and 9.9 -- which is bigger?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"9.11 and 9.9 -- which is bigger?\"\n",
    "prompt = generate_hermes_prompt(user_prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0608a24-7f39-4ed8-9b6b-d551d9d556a6",
   "metadata": {},
   "source": [
    "We use `generate.regex` by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e39197-cf88-4008-8e6d-814dec90a9a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:14.590357Z",
     "iopub.status.busy": "2024-08-08T16:26:14.589904Z",
     "iopub.status.idle": "2024-08-08T16:26:18.016969Z",
     "shell.execute_reply": "2024-08-08T16:26:18.015891Z",
     "shell.execute_reply.started": "2024-08-08T16:26:14.590313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"reasoning\" : [ { \"reasoning_step\" : \"Both 9.11 and 9.9 are decimal numbers.\" }, { \"reasoning_step\" : \"When comparing decimal numbers, we look at the numbers after the decimal point.\" }, { \"reasoning_step\" : \"In this case, 9.11 has the number 1 after the decimal point, while 9.9 has the number 9.\" }, { \"reasoning_step\" : \"Since 1 is greater than 9, 9.11 is greater than 9.9.\" } ], \"conclusion\" : \"9.11 is bigger.\" }'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = generate.regex(model, regex_str)\n",
    "response = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f9afb-83bf-4b48-be71-23e80b83f1e8",
   "metadata": {},
   "source": [
    "We obtain a series of intermediate reasoning steps as well as the conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0062e0-e49f-4034-8606-a6491f8fd154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:18.018100Z",
     "iopub.status.busy": "2024-08-08T16:26:18.017899Z",
     "iopub.status.idle": "2024-08-08T16:26:18.023265Z",
     "shell.execute_reply": "2024-08-08T16:26:18.022596Z",
     "shell.execute_reply.started": "2024-08-08T16:26:18.018083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reasoning_step': 'Both 9.11 and 9.9 are decimal numbers.'},\n",
       " {'reasoning_step': 'When comparing decimal numbers, we look at the numbers after the decimal point.'},\n",
       " {'reasoning_step': 'In this case, 9.11 has the number 1 after the decimal point, while 9.9 has the number 9.'},\n",
       " {'reasoning_step': 'Since 1 is greater than 9, 9.11 is greater than 9.9.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response)\n",
    "json_response[\"reasoning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa12b2b-800d-4bc6-9a07-7847cfd2842f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:26:18.024347Z",
     "iopub.status.busy": "2024-08-08T16:26:18.024047Z",
     "iopub.status.idle": "2024-08-08T16:26:18.049441Z",
     "shell.execute_reply": "2024-08-08T16:26:18.048426Z",
     "shell.execute_reply.started": "2024-08-08T16:26:18.024323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.11 is bigger.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response[\"conclusion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90363279-bce2-4ffd-aa38-e582e8523014",
   "metadata": {},
   "source": [
    "We notice that the 4th reasoning step is wrong `Since 1 is greater than 9, 9.11 is greater than 9.9.`, so we should probably give the model some examples for this particular task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
